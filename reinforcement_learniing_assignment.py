# -*- coding: utf-8 -*-
"""Reinforcement_Learniing_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yYBNxKBT3xsm7MFAN-_NZ0x3SI-GTo42

**Reinforcement Learning: Individual Assignment**

Name: Basma Reda Shaban Abd-Elsalam Abd-Elwahab

Student number: 300327209

Email: babde014@uottawa.ca

#Importing important libraries
"""

!pip install cmake 'gym[atari]' scipy

from numpy import array #to deal with arrays
import math #to deal with math functions
from math import inf
from numpy.linalg import norm
import gym #for developing reinforcement learning environments
import random 
import numpy as np
from IPython.display import clear_output #to produce clear output
from time import sleep #to sleep for a time

"""**Define our reinforcement learning environment**

Taxi-V3 environment>>>>>Teach a Taxi Cab to drive around with Q-Learning
"""

env = gym.make("Taxi-v3")
#env1 = gym.make("MountainCar-v0")

env.reset() #A new environment is generated
env.render()

#print the 6 actions that the taxi can take, and the 500 states of the taxi
print("Action Space {}".format(env.action_space))
print("State Space {}".format(env.observation_space))

env.action_space.n, env.observation_space.n

"""#Problem 1: Turning the code into a module of functions that can use multiple environments"""

#env.s = 328  # set environment to illustration's state
def set_time(env):
  epochs = 0   #set the number of epochs
  penalties = 0 #set the number of penalties
  reward = 0 #set the number of reward
  frames = [] # list of frames for animation
  done = False #set done to false

  while not done:
    action = env.action_space.sample()
    state, reward, done, info = env.step(action)
    if reward == -10:
        penalties += 1
    # Put the rendered frame into dictionary for animation
    frames.append({
        'frame': env.render(mode='ansi'),
        'state': state,
        'action': action,
        'reward': reward
        }
    )

    epochs += 1
    #print the time taken to reach and penalities of the agent    
  print("Timesteps taken: {}".format(epochs))
  print("Penalties incurred: {}".format(penalties))
  return frames

frames = set_time(env)

def print_frames(frames):
    for i, frame in enumerate(frames):
        clear_output(wait=True)
        #print(frame['frame'].getvalue())
        print(frame['frame'])
        print(f"Timestep: {i + 1}")
        print(f"State: {frame['state']}")
        print(f"Action: {frame['action']}")
        print(f"Reward: {frame['reward']}")
        sleep(.1)
        
print_frames(frames)

"""**Start training the agent of Taxi Driver**"""

def TrainAgent(env):
  #define q_table
  q_table = np.zeros([env.observation_space.n, env.action_space.n])
  #define the hyperparameters
  epsilon = 0.5
  alpha = 0.1
  gamma = 0.6
  #plotting the matrix
  all_epochs = []
  all_penalties = []
  table = []
  for i in range(0, 15100):
    state = env.reset()
    epochs = 0
    penalties = 0
    reward = 0
    #set the done state to false
    done = False
    #while loop to check the state of the agent
    while done == False:
    #select the action:
    # take a random number
        if random.uniform(0, 1) < epsilon: 
          action = env.action_space.sample() # Explore action space
        else:
          action = np.argmax(q_table[state,:]) # Exploit learned values
            # Then we perform the action and receive the feedback from the environment
          new_state, reward, done, info = env.step(action)
            # Finally we learn from the experience by updating the Q-value of the selected action
          update = reward + (gamma*np.max(q_table[new_state,:])) - q_table[state, action]
          q_table[state,action] += alpha*update 
          if ( reward == -10):
            penalties += 1
          state = new_state
          epochs += 1
    if( i % 100 ==0):
      clear_output(wait=True)
      print(f"Episode: {i}")

  return q_table
q_table = TrainAgent(env)

print("Q table values are:\n", q_table )

"""**Evaluate and launch the agent**"""

def launch_game(q_table, env):
  #define total epochs and penalities
  total_epochs = 0
  total_penalties = 0
  #set the number of episodes to 100
  episodes = 100
  for x in range(episodes):
    # define initial state
    state = env.reset()
    epochs = 0
    penalties = 0 
    reward = 0
   #set done state ro false
    done = False
    while done == False:
        # Take the action (index) with the maximum expected discounted future reward given that state
        action = np.argmax(q_table[state,:])
        state, reward, done, info = env.step(action)
        if reward == -10:
              penalties += 1

        epochs += 1
        #increment total penalities of the agent and total epochs 
        total_penalties += penalties
        total_epochs += epochs
  print(f"Average timesteps per episode: {total_epochs / episodes}")
  print(f"Average penalties per episode: {total_penalties / episodes}")
launch_game(q_table, env)

"""#Problem 2: Tuning alpha, gamma, and/or epsilon using a decay over episodes"""

def HyperParametersTuning(env,epsilon, alpha, gamma):
  q_table = np.zeros([env.observation_space.n, env.action_space.n])
  #define the hyperparameters
  epsilon = epsilon
  alpha = alpha
  gamma = gamma
  
 #plotting the matrix  
  all_epochs = []
  all_penalties = []
  table = []
  for i in range(1, 15100):
    state = env.reset()
    epochs = 0
    penalties = 0
    reward = 0      
       #set the done state to false
    done = False
    #while loop to check the state of the agent
    while not done:
      if random.uniform(0, 1) < epsilon:
         action = env.action_space.sample() 
      else:
          action = np.argmax(q_table[state]) 
          next_state, reward, done, info = env.step(action) 
          
          old_value = q_table[state, action]
          new_state = np.max(q_table[next_state])
          
          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * new_state)
          q_table[state, action] = new_value
          table.append(q_table[state, action])
          epsilon = epsilon - (0.0001*epsilon)
          alpha = alpha - (0.0001*alpha)
          gamma = gamma - (0.0001*gamma)
          if epsilon<= 0:
            epsilon = 0.1
          if alpha<=0:
            alpha = 0.1
          if gamma<=0:
            gamma = 0.6

          if reward == -10:
              penalties += 1

          state = next_state
          epochs += 1
          
      if i % 100 == 0:
          clear_output(wait=True)
          print(f"Episode: {i}")
  return q_table

new_env1 = gym.make("Taxi-v3")
new_q_table_1 = HyperParametersTuning(new_env1,0.1,0.1,0.1)
launch_game(new_q_table_1,new_env1)

"""#Problem 3: Implementing a grid search to discover the best hyperparameters"""

def TrainingGridSearch(env,epsilon, alpha, gamma):
  #define q_table
  q_table = np.zeros([env.observation_space.n, env.action_space.n])
  #define the hyperparameters
  epsilon = 0.1
  alpha = 0.1
  gamma = 0.6
  #plotting the matrix
  all_epochs = []
  all_penalties = []
  table = []
  for i in range(0, 15100):
    state = env.reset()
    epochs = 0
    penalties = 0
    reward = 0
    #set the done state to false
    done = False
    #while loop to check the state of the agent
    while done == False:
    #select the action:
    # take a random number
        if random.uniform(0, 1) < epsilon: 
          action = env.action_space.sample() # Explore action space
        else:
          action = np.argmax(q_table[state,:]) # Exploit learned values
            # Then we perform the action and receive the feedback from the environment
          new_state, reward, done, info = env.step(action)
            # Finally we learn from the experience by updating the Q-value of the selected action
          update = reward + (gamma*np.max(q_table[new_state,:])) - q_table[state, action]
          q_table[state,action] += alpha*update 
          if ( reward == -10):
            penalties += 1
          state = new_state
          epochs += 1
    if( i % 100 ==0):
      clear_output(wait=True)
      print(f"Episode: {i}")

  return q_table,epsilon, alpha, gamma

def EvaluateGridSearch(q_table, env):
  #define total epochs and penalities
  total_epochs = 0
  total_penalties = 0
  #set the number of episodes to 100
  episodes = 100
  for x in range(episodes):
    # define initial state
    state = env.reset()
    epochs = 0
    penalties = 0 
    reward = 0
   #set done state ro false
    done = False
    while done == False:
        # Take the action (index) with the maximum expected discounted future reward given that state
        action = np.argmax(q_table[state,:])
        state, reward, done, info = env.step(action)
        if reward == -10:
              penalties += 1

        epochs += 1
        #increment total penalities of the agent and total epochs 
        total_penalties += penalties
        total_epochs += epochs
        TAvg = total_epochs / episodes
        pAvg = total_penalties / episodes
  print(f"Average timesteps per episode: {TAvg}")
  print(f"Average penalties per episode: {pAvg}")
  return TAvg, pAvg

def grid_search(parm,env):
  time_steps = 15000
  penalties = 15000
  GridSearchHyperParameters = parm
  for i in GridSearchHyperParameters['alpha']:
    for j in GridSearchHyperParameters['gamma']:
      for k in GridSearchHyperParameters['epsilon']:
        q_table,alpha,gamma,epsilon = TrainingGridSearch(env,alpha=i,gamma=j,epsilon=k)
        TAvg,pAvg = EvaluateGridSearch(q_table,env)
        if TAvg<= time_steps:
          if pAvg <= penalties:
            time_steps = TAvg
            penalties = pAvg
            bestparameter = {'alpha':alpha,'gamma':gamma,'epsilon':epsilon,'Time':TAvg,'penalties':pAvg}
  return bestparameter

GridSearchHyperParameters = {'alpha':[0.1,0.2,0.3],'gamma':[0.1,0.2,0.3],'epsilon':[0.1,0.2,0.3]}
grid_search(GridSearchHyperParameters,env)

GridSearchHyperParameters = {'alpha':[0.4,0.5,0.6],'gamma':[0.4,0.5,0.6],'epsilon':[0.4,0.5,0.6]}
grid_search(GridSearchHyperParameters,env)

GridSearchHyperParameters = {'alpha':[0.7,0.8,0.9],'gamma':[0.7,0.8,0.9],'epsilon':[0.7,0.8,0.9]}
grid_search(GridSearchHyperParameters,env)

"""#Apply the training and evaluation agent functions to a different Q learning environment called 'FrozenLake-v0' environment"""

env2 = gym.make("FrozenLake-v0")

q_table2 = TrainAgent(env2)

launch_game(q_table2, env2)

new_env2 = gym.make("FrozenLake-v0")
new_q_table_2 = HyperParametersTuning(new_env2,0.1,0.1,0.1)
launch_game(new_q_table_2,new_env2)

GridSearchHyperParameters = {'alpha':[0.1,0.2,0.3],'gamma':[0.1,0.2,0.3],'epsilon':[0.1,0.2,0.3]}
grid_search(GridSearchHyperParameters,env2)

GridSearchHyperParameters = {'alpha':[0.4,0.5,0.6],'gamma':[0.4,0.5,0.6],'epsilon':[0.4,0.5,0.6]}
grid_search(GridSearchHyperParameters,env2)

GridSearchHyperParameters = {'alpha':[0.7,0.8,0.9],'gamma':[0.7,0.8,0.9],'epsilon':[0.7,0.8,0.9]}
grid_search(GridSearchHyperParameters,env2)