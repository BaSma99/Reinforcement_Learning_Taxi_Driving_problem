{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_Learniing_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Reinforcement Learning: Individual Assignment**"
      ],
      "metadata": {
        "id": "v3cYy-vPdPWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Basma Reda Shaban Abd-Elsalam Abd-Elwahab\n"
      ],
      "metadata": {
        "id": "b4g4HvvXdU3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student number: 300327209"
      ],
      "metadata": {
        "id": "1lG_ISOldXO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Email: babde014@uottawa.ca"
      ],
      "metadata": {
        "id": "QGGUuucZdZAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing important libraries\n"
      ],
      "metadata": {
        "id": "GgXEykwAhzLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlzQG5xxN6H4",
        "outputId": "c886f736-86a7-4ab9-e1aa-a4d07daeaf68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.4)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gFw0XVqQ1ViW"
      },
      "outputs": [],
      "source": [
        "from numpy import array #to deal with arrays\n",
        "import math #to deal with math functions\n",
        "from math import inf\n",
        "from numpy.linalg import norm\n",
        "import gym #for developing reinforcement learning environments\n",
        "import random \n",
        "import numpy as np\n",
        "from IPython.display import clear_output #to produce clear output\n",
        "from time import sleep #to sleep for a time \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define our reinforcement learning environment**"
      ],
      "metadata": {
        "id": "IbFHUWHS-xs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taxi-V3 environment>>>>>Teach a Taxi Cab to drive around with Q-Learning"
      ],
      "metadata": {
        "id": "-FUpeOHA_OPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "#env1 = gym.make(\"MountainCar-v0\")"
      ],
      "metadata": {
        "id": "cc6p123V1siI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() #A new environment is generated\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slgs37bJ1zIc",
        "outputId": "6ee9a05e-9510-4dd8-849e-f8ff60beeae4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[43mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print the 6 actions that the taxi can take, and the 500 states of the taxi\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJlhWwrB13Kt",
        "outputId": "632c2260-a326-4b6f-ddc9-97f795c11e52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.n, env.observation_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BOqNMPt1_uM",
        "outputId": "d2c33681-837b-49f9-98fd-33cf53aec56a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1: Turning the code into a module of functions that can use multiple environments"
      ],
      "metadata": {
        "id": "DFV1-d5ZE5IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#env.s = 328  # set environment to illustration's state\n",
        "def set_time(env):\n",
        "  epochs = 0   #set the number of epochs\n",
        "  penalties = 0 #set the number of penalties\n",
        "  reward = 0 #set the number of reward\n",
        "  frames = [] # list of frames for animation\n",
        "  done = False #set done to false\n",
        "\n",
        "  while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    # Put the rendered frame into dictionary for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    #print the time taken to reach and penalities of the agent    \n",
        "  print(\"Timesteps taken: {}\".format(epochs))\n",
        "  print(\"Penalties incurred: {}\".format(penalties))\n",
        "  return frames\n"
      ],
      "metadata": {
        "id": "F3FqkcVRF9cO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = set_time(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsTu--J426kc",
        "outputId": "d8c74fdc-eec2-44e8-f48c-0f7be5c5e7b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 200\n",
            "Penalties incurred: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        #print(frame['frame'].getvalue())\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CieKMTtsGFro",
        "outputId": "b1dfda36-9118-47b1-c9d9-1734e71761f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "Timestep: 200\n",
            "State: 141\n",
            "Action: 3\n",
            "Reward: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start training the agent of Taxi Driver**"
      ],
      "metadata": {
        "id": "f3AINgvZnBju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainAgent(env):\n",
        "  #define q_table\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "  #define the hyperparameters\n",
        "  epsilon = 0.5\n",
        "  alpha = 0.1\n",
        "  gamma = 0.6\n",
        "  #plotting the matrix\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  table = []\n",
        "  for i in range(0, 15100):\n",
        "    state = env.reset()\n",
        "    epochs = 0\n",
        "    penalties = 0\n",
        "    reward = 0\n",
        "    #set the done state to false\n",
        "    done = False\n",
        "    #while loop to check the state of the agent\n",
        "    while done == False:\n",
        "    #select the action:\n",
        "    # take a random number\n",
        "        if random.uniform(0, 1) < epsilon: \n",
        "          action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "          action = np.argmax(q_table[state,:]) # Exploit learned values\n",
        "            # Then we perform the action and receive the feedback from the environment\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "            # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "          update = reward + (gamma*np.max(q_table[new_state,:])) - q_table[state, action]\n",
        "          q_table[state,action] += alpha*update \n",
        "          if ( reward == -10):\n",
        "            penalties += 1\n",
        "          state = new_state\n",
        "          epochs += 1\n",
        "    if( i % 100 ==0):\n",
        "      clear_output(wait=True)\n",
        "      print(f\"Episode: {i}\")\n",
        "\n",
        "  return q_table\n",
        "q_table = TrainAgent(env)"
      ],
      "metadata": {
        "id": "XT7tucu52UwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f4d848-910e-4d63-f5da-4d8b37e1d8a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q table values are:\\n\", q_table )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpKAlXdX2nat",
        "outputId": "fd4d1eb4-a2b2-4b86-b4ba-f2bd0edc4524"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q table values are:\n",
            " [[ 0.          0.          0.          0.          0.          0.        ]\n",
            " [-2.28873104 -2.29275445 -2.28980572 -2.29275445 -2.27325184 -2.8816    ]\n",
            " [-1.72727388 -1.73474572 -1.73864537 -1.73474572 -0.7504     -1.96      ]\n",
            " ...\n",
            " [-1.14478405 -1.09266831 -1.14478405 -1.16369882 -1.96       -1.96      ]\n",
            " [-1.9249516  -1.93872851 -1.9249516  -1.92446755 -1.96       -1.96      ]\n",
            " [-0.196      -0.196      -0.196       0.24968    -1.         -1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate and launch the agent**"
      ],
      "metadata": {
        "id": "gMOwmCrztS2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def launch_game(q_table, env):\n",
        "  #define total epochs and penalities\n",
        "  total_epochs = 0\n",
        "  total_penalties = 0\n",
        "  #set the number of episodes to 100\n",
        "  episodes = 100\n",
        "  for x in range(episodes):\n",
        "    # define initial state\n",
        "    state = env.reset()\n",
        "    epochs = 0\n",
        "    penalties = 0 \n",
        "    reward = 0\n",
        "   #set done state ro false\n",
        "    done = False\n",
        "    while done == False:\n",
        "        # Take the action (index) with the maximum expected discounted future reward given that state\n",
        "        action = np.argmax(q_table[state,:])\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "        #increment total penalities of the agent and total epochs \n",
        "        total_penalties += penalties\n",
        "        total_epochs += epochs\n",
        "  print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "  print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "launch_game(q_table, env)"
      ],
      "metadata": {
        "id": "XzHEu6Rl2pyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3092334-27bd-48b4-ebc6-13cd29848c89"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average timesteps per episode: 300.33\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 2: Tuning alpha, gamma, and/or epsilon using a decay over episodes"
      ],
      "metadata": {
        "id": "W3aBGWQZFGfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HyperParametersTuning(env,epsilon, alpha, gamma):\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "  #define the hyperparameters\n",
        "  epsilon = epsilon\n",
        "  alpha = alpha\n",
        "  gamma = gamma\n",
        "  \n",
        " #plotting the matrix  \n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  table = []\n",
        "  for i in range(1, 15100):\n",
        "    state = env.reset()\n",
        "    epochs = 0\n",
        "    penalties = 0\n",
        "    reward = 0      \n",
        "       #set the done state to false\n",
        "    done = False\n",
        "    #while loop to check the state of the agent\n",
        "    while not done:\n",
        "      if random.uniform(0, 1) < epsilon:\n",
        "         action = env.action_space.sample() \n",
        "      else:\n",
        "          action = np.argmax(q_table[state]) \n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          \n",
        "          old_value = q_table[state, action]\n",
        "          new_state = np.max(q_table[next_state])\n",
        "          \n",
        "          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * new_state)\n",
        "          q_table[state, action] = new_value\n",
        "          table.append(q_table[state, action])\n",
        "          epsilon = epsilon - (0.0001*epsilon)\n",
        "          alpha = alpha - (0.0001*alpha)\n",
        "          gamma = gamma - (0.0001*gamma)\n",
        "          if epsilon<= 0:\n",
        "            epsilon = 0.1\n",
        "          if alpha<=0:\n",
        "            alpha = 0.1\n",
        "          if gamma<=0:\n",
        "            gamma = 0.6\n",
        "\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "          \n",
        "      if i % 100 == 0:\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Episode: {i}\")\n",
        "  return q_table"
      ],
      "metadata": {
        "id": "ubZ9iAOVFTVv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env1 = gym.make(\"Taxi-v3\")\n",
        "new_q_table_1 = HyperParametersTuning(new_env1,0.1,0.1,0.1)\n",
        "launch_game(new_q_table_1,new_env1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxJjS6C-R8EU",
        "outputId": "520ae46e-3b1d-4791-b3b8-6a691d810ffd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 20100.0\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 3: Implementing a grid search to discover the best hyperparameters"
      ],
      "metadata": {
        "id": "laQKPkDI31xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainingGridSearch(env,epsilon, alpha, gamma):\n",
        "  #define q_table\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "  #define the hyperparameters\n",
        "  epsilon = 0.1\n",
        "  alpha = 0.1\n",
        "  gamma = 0.6\n",
        "  #plotting the matrix\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  table = []\n",
        "  for i in range(0, 15100):\n",
        "    state = env.reset()\n",
        "    epochs = 0\n",
        "    penalties = 0\n",
        "    reward = 0\n",
        "    #set the done state to false\n",
        "    done = False\n",
        "    #while loop to check the state of the agent\n",
        "    while done == False:\n",
        "    #select the action:\n",
        "    # take a random number\n",
        "        if random.uniform(0, 1) < epsilon: \n",
        "          action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "          action = np.argmax(q_table[state,:]) # Exploit learned values\n",
        "            # Then we perform the action and receive the feedback from the environment\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "            # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "          update = reward + (gamma*np.max(q_table[new_state,:])) - q_table[state, action]\n",
        "          q_table[state,action] += alpha*update \n",
        "          if ( reward == -10):\n",
        "            penalties += 1\n",
        "          state = new_state\n",
        "          epochs += 1\n",
        "    if( i % 100 ==0):\n",
        "      clear_output(wait=True)\n",
        "      print(f\"Episode: {i}\")\n",
        "\n",
        "  return q_table,epsilon, alpha, gamma"
      ],
      "metadata": {
        "id": "tAX_2_0j4FVZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EvaluateGridSearch(q_table, env):\n",
        "  #define total epochs and penalities\n",
        "  total_epochs = 0\n",
        "  total_penalties = 0\n",
        "  #set the number of episodes to 100\n",
        "  episodes = 100\n",
        "  for x in range(episodes):\n",
        "    # define initial state\n",
        "    state = env.reset()\n",
        "    epochs = 0\n",
        "    penalties = 0 \n",
        "    reward = 0\n",
        "   #set done state ro false\n",
        "    done = False\n",
        "    while done == False:\n",
        "        # Take the action (index) with the maximum expected discounted future reward given that state\n",
        "        action = np.argmax(q_table[state,:])\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "        #increment total penalities of the agent and total epochs \n",
        "        total_penalties += penalties\n",
        "        total_epochs += epochs\n",
        "        TAvg = total_epochs / episodes\n",
        "        pAvg = total_penalties / episodes\n",
        "  print(f\"Average timesteps per episode: {TAvg}\")\n",
        "  print(f\"Average penalties per episode: {pAvg}\")\n",
        "  return TAvg, pAvg"
      ],
      "metadata": {
        "id": "qrvAVwPW4esK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(parm,env):\n",
        "  time_steps = 15000\n",
        "  penalties = 15000\n",
        "  GridSearchHyperParameters = parm\n",
        "  for i in GridSearchHyperParameters['alpha']:\n",
        "    for j in GridSearchHyperParameters['gamma']:\n",
        "      for k in GridSearchHyperParameters['epsilon']:\n",
        "        q_table,alpha,gamma,epsilon = TrainingGridSearch(env,alpha=i,gamma=j,epsilon=k)\n",
        "        TAvg,pAvg = EvaluateGridSearch(q_table,env)\n",
        "        if TAvg<= time_steps:\n",
        "          if pAvg <= penalties:\n",
        "            time_steps = TAvg\n",
        "            penalties = pAvg\n",
        "            bestparameter = {'alpha':alpha,'gamma':gamma,'epsilon':epsilon,'Time':TAvg,'penalties':pAvg}\n",
        "  return bestparameter"
      ],
      "metadata": {
        "id": "LqQCh_cx85zN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchHyperParameters = {'alpha':[0.1,0.2,0.3],'gamma':[0.1,0.2,0.3],'epsilon':[0.1,0.2,0.3]}\n",
        "grid_search(GridSearchHyperParameters,env)"
      ],
      "metadata": {
        "id": "vhCb-EoOaLS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd1a7a7-8d85-4cee-bde2-536a6a83c615"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 696.65\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Time': 289.95, 'alpha': 0.1, 'epsilon': 0.6, 'gamma': 0.1, 'penalties': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchHyperParameters = {'alpha':[0.4,0.5,0.6],'gamma':[0.4,0.5,0.6],'epsilon':[0.4,0.5,0.6]}\n",
        "grid_search(GridSearchHyperParameters,env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HtgZ03n5v7t",
        "outputId": "a10c3c83-e7b1-4b48-8904-70ae72999c25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 695.34\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Time': 294.45, 'alpha': 0.1, 'epsilon': 0.6, 'gamma': 0.1, 'penalties': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchHyperParameters = {'alpha':[0.7,0.8,0.9],'gamma':[0.7,0.8,0.9],'epsilon':[0.7,0.8,0.9]}\n",
        "grid_search(GridSearchHyperParameters,env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAHgXswE90B9",
        "outputId": "b82ada74-d407-4c42-abc7-cbf94ec32a8b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 292.98\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Time': 93.86, 'alpha': 0.1, 'epsilon': 0.6, 'gamma': 0.1, 'penalties': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply the training and evaluation agent functions to a different Q learning environment called 'FrozenLake-v0' environment"
      ],
      "metadata": {
        "id": "VmAW021oZbxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env2 = gym.make(\"FrozenLake-v0\")"
      ],
      "metadata": {
        "id": "y5l24xBNMwJb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table2 = TrainAgent(env2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De7nLUvVWH6l",
        "outputId": "760a33f2-f652-45d5-a36f-f451d4fca322"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "launch_game(q_table2, env2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_JLt0SeYAKw",
        "outputId": "e634ae35-c21f-4f61-eb77-09092a8ccf5e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average timesteps per episode: 258.87\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env2 = gym.make(\"FrozenLake-v0\")\n",
        "new_q_table_2 = HyperParametersTuning(new_env2,0.1,0.1,0.1)\n",
        "launch_game(new_q_table_2,new_env2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv6EBM3WZYGe",
        "outputId": "c2bd68a5-e79c-4c34-bf62-a9a5f553cada"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 248.34\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchHyperParameters = {'alpha':[0.1,0.2,0.3],'gamma':[0.1,0.2,0.3],'epsilon':[0.1,0.2,0.3]}\n",
        "grid_search(GridSearchHyperParameters,env2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vDqlqhHbvOk",
        "outputId": "7662d6d3-30f2-4b14-fabc-6aa2a9abd668"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 316.91\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Time': 149.1, 'alpha': 0.1, 'epsilon': 0.6, 'gamma': 0.1, 'penalties': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchHyperParameters = {'alpha':[0.4,0.5,0.6],'gamma':[0.4,0.5,0.6],'epsilon':[0.4,0.5,0.6]}\n",
        "grid_search(GridSearchHyperParameters,env2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiEy-EGkcCEC",
        "outputId": "5cab8932-904f-46eb-f5b0-fe03fd1fb146"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 254.93\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Time': 197.71, 'alpha': 0.1, 'epsilon': 0.6, 'gamma': 0.1, 'penalties': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchHyperParameters = {'alpha':[0.7,0.8,0.9],'gamma':[0.7,0.8,0.9],'epsilon':[0.7,0.8,0.9]}\n",
        "grid_search(GridSearchHyperParameters,env2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_1v7iyRdNQi",
        "outputId": "a4f99084-f046-4040-ce60-ee6d6e3a653a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 15000\n",
            "Average timesteps per episode: 281.44\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Time': 192.58, 'alpha': 0.1, 'epsilon': 0.6, 'gamma': 0.1, 'penalties': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}